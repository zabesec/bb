import json
import os
import random
import re
import subprocess
import sys
import time

import requests

COOKIES = []
cookie_index = 0
PROGRESS_FILE = '/app/output/progress.json'
SCANNED_FILE = '/app/output/scanned.txt'

SEARCH_QUERIES = [
    {'hash': '2141724739', 'name': 'Juniper Networks'},
    {'query': 'http.html:"wp-content/plugins/elementor-pro"', 'name': 'Elementor Pro'},
    {'query': 'http.title:"FortiNAC"', 'name': 'FortiNAC'},
    {'query': 'http.title:"CrushFTP"', 'name': 'CrushFTP'},
    {'query': 'http.html:"DEBUG = True"', 'name': 'Django Debug'},
    {'query': 'http.title:"ColdFusion Administrator"', 'name': 'ColdFusion Admin'}
]

def fetch_from_url(url):
    cmd = ['curl', '-s', url]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode != 0:
            return []
        items = []
        for line in result.stdout.split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                items.append(line)
        return items
    except Exception as e:
        return []

def load_cookies(cookie_url):
    global COOKIES
    COOKIES = []
    cookies = fetch_from_url(cookie_url)
    if cookies:
        COOKIES.extend(cookies)
        print(f"[+] Loaded {len(cookies)} cookies")
        return True
    return False

def clean_domain(domain):
    domain = domain.lower().strip()
    domain = re.sub(r'^https?://', '', domain)
    domain = re.sub(r'/.*$', '', domain)
    domain = domain.split(':')[0]

    if re.match(r'^[\d\.]+$', domain):
        return None

    if not re.search(r'[a-zA-Z]', domain):
        return None

    parts = domain.split('.')
    if len(parts) < 2:
        return None

    return domain

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r') as f:
                return json.load(f)
        except:
            return {'last_index': 0, 'total_found': 0}
    return {'last_index': 0, 'total_found': 0}

def save_progress(index, total_found):
    os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({'last_index': index, 'total_found': total_found}, f)

def mark_scanned(domain):
    os.makedirs(os.path.dirname(SCANNED_FILE), exist_ok=True)
    with open(SCANNED_FILE, 'a') as f:
        f.write(f"{domain}\n")

def load_domains(targets_url):
    all_lines = fetch_from_url(targets_url)
    if not all_lines:
        return []

    valid_domains = set()
    for domain in all_lines:
        cleaned = clean_domain(domain)
        if cleaned:
            valid_domains.add(cleaned)

    unique_domains = sorted(list(valid_domains))
    print(f"[+] Total lines: {len(all_lines)}")
    print(f"[+] Valid domains (with subdomains): {len(unique_domains)}")
    return unique_domains

def get_next_cookie():
    global cookie_index
    if not COOKIES:
        return None
    cookie = COOKIES[cookie_index]
    cookie_index = (cookie_index + 1) % len(COOKIES)
    return cookie

def scrape_shodan(query_obj, domain):
    cookie = get_next_cookie()
    if not cookie:
        return False, None, None

    if 'hash' in query_obj:
        search_query = f"http.favicon.hash:{query_obj['hash']} Ssl:{domain}"
    else:
        search_query = f"{query_obj['query']} ssl:{domain}"

    url = f"https://www.shodan.io/search?query={search_query.replace(':', '%3A').replace(' ', '+').replace(chr(34), '%22')}"
    print(f"[+] Searching: {domain}")
    print(f"[+] Query Type: {query_obj['name']}")
    print(f"[+] Query: {search_query}")
    print(f"[+] URL: {url}")
    print(f"[+] Cookie: {cookie}")

    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')

    cmd = [
        'curl', '-s',
        '-H', f'Cookie: {cookie}',
        '-H', 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:145.0) Gecko/20100101 Firefox/145.0',
        '-H', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        url
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        html = result.stdout
        if not html:
            return False, None, None

        if 'rate limit' in html.lower() or 'too many requests' in html.lower():
            print(f"[!] RATE LIMITED - Waiting 15 minutes...")
            time.sleep(900)

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            html = result.stdout

            if 'rate limit' in html.lower() or 'too many requests' in html.lower():
                print(f"[!] STILL RATE LIMITED - Sending alert and stopping...")
                send_discord_alert(webhook_url, "rate_limit", "Still rate limited after 15 minute wait. Stopping script.", domain)
                sys.exit(1)

        if 'Please log in to use search filters' in html:
            print(f"[!] SESSION EXPIRED - Cookie invalid")
            send_discord_alert(webhook_url, "session_expired", f"Cookie session expired or invalid:\n`{cookie[:50]}...`", domain)
            print(f"[!] STOPPING SCRIPT - Fix cookie and restart")
            sys.exit(1)

        if 'blocked' in html.lower() or 'captcha' in html.lower():
            print(f"[!] BLOCKED - IP or cookie flagged")
            send_discord_alert(webhook_url, "blocked", "IP or cookie has been blocked/flagged by Shodan", domain)
            print(f"[!] STOPPING SCRIPT - Check IP/cookie and restart")
            sys.exit(1)

        if 'No results found' in html:
            return False, url, None

        if '<div class="result">' in html or 'search-result' in html:
            return True, url, query_obj['name']

        return False, url, None
    except:
        return False, None, None

def send_discord_webhook(webhook_url, domain, search_url, query_name):
    embed = {
        "title": "ðŸŽ¯ Results Found",
        "description": f"**Domain:** `{domain}`\n**Query Type:** {query_name}",
        "fields": [
            {
                "name": "Search URL",
                "value": f"[View on Shodan]({search_url})",
                "inline": False
            }
        ]
    }
    payload = {
        "embeds": [embed]
    }
    try:
        response = requests.post(webhook_url, json=payload, timeout=10)
        if response.status_code == 204:
            print(f"[+] Sent to Discord: {domain} - {query_name}")
            return True
        else:
            print(f"[-] Discord webhook failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"[-] Discord error: {e}")
        return False

def send_discord_alert(webhook_url, alert_type, message, domain=None):
    embed = {
        "title": f"âš ï¸ Alert: {alert_type.replace('_', ' ').title()}",
        "description": message,
        "timestamp": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
    }

    if domain:
        embed["fields"] = [{"name": "Domain", "value": domain, "inline": False}]

    payload = {"embeds": [embed]}

    try:
        requests.post(webhook_url, json=payload, timeout=10)
    except:
        pass

def main():
    print("="*60)
    print("Multi-Query Shodan Scanner")
    print("="*60)
    cookie_url = os.getenv('COOKIE_URL')
    targets_url = os.getenv('TARGETS_URL')
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not cookie_url or not targets_url or not webhook_url:
        print("[-] Missing environment variables:")
        print("    COOKIE_URL, TARGETS_URL, DISCORD_WEBHOOK_URL")
        sys.exit(1)
    if not load_cookies(cookie_url):
        print("[-] Failed to load cookies")
        sys.exit(1)
    domains = load_domains(targets_url)
    if not domains:
        print("[-] No domains loaded")
        sys.exit(1)
    progress = load_progress()
    start_index = progress['last_index']
    total_results = progress['total_found']
    if start_index > 0:
        print(f"[+] Resuming from index {start_index}/{len(domains)}")
        print(f"[+] Previous results found: {total_results}")

    print(f"[+] Running {len(SEARCH_QUERIES)} query types per domain")

    for i in range(start_index, len(domains)):
        domain = domains[i]
        print(f"\n[{i+1}/{len(domains)}] Processing: {domain}")

        for query_obj in SEARCH_QUERIES:
            has_results, search_url, query_name = scrape_shodan(query_obj, domain)
            if has_results and search_url:
                print(f"    Results found for {query_obj['name']}")
                send_discord_webhook(webhook_url, domain, search_url, query_name)
                total_results += 1
            else:
                print(f"    No results for {query_obj['name']}")

            if query_obj != SEARCH_QUERIES[-1]:
                delay = random.uniform(5, 10)
                print(f"[*] Waiting {delay:.1f}s before next query...")
                time.sleep(delay)

        mark_scanned(domain)
        save_progress(i + 1, total_results)

        if i < len(domains) - 1:
            delay = random.uniform(7, 10)
            print(f"[*] Waiting {delay:.1f}s before next domain...")
            time.sleep(delay)
    print("\n" + "="*60)
    print(f"[+] Complete: {total_results} total results found")
    print(f"[+] Scanned: {len(domains)} domains")
    print("="*60)

if __name__ == "__main__":
    main()
